# -*- coding: utf-8 -*-
"""Create custom pipeline - NerDL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EXDhLb1tlcZ_1g7bRXMV9uf_TMUTdX33

Show how to use pretrained assertion status
"""

from google.colab import drive
drive.mount("/content/gdrive/")
import sys
sys.path.append('/content/gdrive/My Drive/Colab Notebooks/SparkNLP/utils')
!ls

!cp "gdrive/My Drive/Colab Notebooks/SparkNLP/utils/con_rest_train.bio" "con_rest_train.bio"
!cp "gdrive/My Drive/Colab Notebooks/SparkNLP/utils/glove.6B.300d.txt" "glove.6B.300d.txt"
!cp "gdrive/My Drive/Colab Notebooks/SparkNLP/utils/blstm_17_300_128_37.pb" "blstm_17_300_128_37.pb"

!ls

import os

# Install java
! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["PATH"] = os.environ["JAVA_HOME"] + "/bin:" + os.environ["PATH"]
! java -version

# Install pyspark
! pip install --ignore-installed pyspark==2.4.3

# Install Spark NLP
! pip install --ignore-installed spark-nlp==2.2.2

import sys

from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel

import sparknlp
from sparknlp.annotator import *
from sparknlp.common import *
from sparknlp.base import *
from sparknlp.pretrained import ResourceDownloader

from pathlib import Path

if sys.version_info[0] < 3:
    from urllib import urlretrieve
else:
    from urllib.request import urlretrieve

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Global DEMO - Spark NLP Enterprise 2.3.4") \
    .master("local[*]") \
    .config("spark.rdd.compress","true") \
    .config("spark.driver.memory","8G") \
    .config("spark.driver.maxResultSize", "2G") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.kryoserializer.buffer.max", "600M") \
    .config("spark.jars.packages", "JohnSnowLabs:spark-nlp:2.3.4") \
    .getOrCreate()
#spark = sparknlp.start()

print("Spark NLP version: ", sparknlp.version())
print("Apache Spark version: ", spark.version)

"""Create some data for testing purposes"""

from pyspark.sql import Row
R = Row('sentence', 'start', 'end')
test_data = spark.createDataFrame([R('Peter is a good person, and he was working at IBM',0,1)])

"""Create a custom pipeline"""

!ls
from sparknlp.training import CoNLL
training_data = CoNLL().readDataset(spark, 'con_rest_train.bio')
training_data.show()

import time

documentAssembler = DocumentAssembler() \
    .setInputCol("text") \
    .setOutputCol("document")

tokenizer = Tokenizer() \
    .setInputCols(["document"]) \
    .setOutputCol("token")

embeddings = WordEmbeddings() \
        .setInputCols(["document", "token"])\
        .setOutputCol("embeddings")\
        .setEmbeddingsSource("glove.6B.300d.txt", 300, "text")

#spell = NorvigSweetingModel.pretrained() \
#    .setInputCols(["token"]) \
#    .setOutputCol("spell")

ner_dl = NerDLApproach()\
    .setInputCols(["document", "token","embeddings"])\
    .setLabelColumn("label")\
    .setOutputCol("ner")\
    .setMaxEpochs(2)\
    .setRandomSeed(0)\
    .setVerbose(2)\
    .setBatchSize(9)\
    .setGraphFolder("gdrive/My Drive/Colab Notebooks/SparkNLP/utils/Graphs")\
    .setEnableOutputLogs(True)

converter = NerConverter()\
  .setInputCols(["document", "token", "ner"])\
  .setOutputCol("ner_converter")

finisher = Finisher() \
    .setInputCols(["ner", "ner_converter"]) \
    .setIncludeMetadata(True)

pipeline_fast_dl = Pipeline(stages = [
    documentAssembler, 
    tokenizer, 
    embeddings, 
    ner_dl,
    converter,
    finisher])

"""Now let's use these pipelines and see the results"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# start = time.time()
# print("Start fitting")
# prediction_model = pipeline_fast_dl.fit(training_data)
# print("Fitting is ended")
# print (time.time() - start)

prediction_data = spark.createDataFrame([["Maria is a nice place."],["any bbq places open before 5 nearby"]]).toDF("text")
prediction_data.show()
prediction_model.transform(prediction_data).show(truncate=False)

prediction_model.write().overwrite().save("ner_dl_model")

!cp -r "ner_dl_model" "gdrive/My Drive/Colab Notebooks/SparkNLP/utils/ner_dl_model_base"

from pyspark.ml import PipelineModel, Pipeline

loaded_prediction_model = PipelineModel.read().load("ner_dl_model")

prediction_data = spark.createDataFrame([["Maria is a nice place."],["any bbq places open before 5 nearby"]]).toDF("text")
prediction_data.show()
#loaded_prediction_model.transform(prediction_data).show(truncate=False)
prediction = loaded_prediction_model.transform(prediction_data)
prediction.select("finished_ner_metadata").show(truncate=False)
prediction.select("finished_ner").show(truncate=False)
prediction.select("finished_ner_converter_metadata").show(truncate=False)
prediction.select("finished_ner_converter").show(truncate=False)
#prediction.select("ner").show(truncate=False)

from sparknlp.base import LightPipeline

lp = LightPipeline(loaded_prediction_model)
result = lp.annotate("Peter is a good person.")
for e in list(zip(result['token'], result['ner']))[:10]:
    print(e)

for stage in loaded_prediction_model.stages:
    print(stage)
print(loaded_prediction_model.stages[-1].stages)